{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('logo.jpg', <http.client.HTTPMessage at 0x2798ed8ab50>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "imageLocation = bs.find('a', {'id': 'logo'}).find('img')['src']\n",
    "urlretrieve (imageLocation, 'logo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://pythonscraping.com/misc/jquery.js?v=1.4.4\n",
      "http://pythonscraping.com/misc/jquery.once.js?v=1.2\n",
      "http://pythonscraping.com/misc/drupal.js?q4na2g\n",
      "http://pythonscraping.com/sites/all/themes/skeletontheme/js/jquery.mobilemenu.js?q4na2g\n",
      "http://pythonscraping.com/sites/all/modules/google_analytics/googleanalytics.js?q4na2g\n",
      "http://pythonscraping.com/sites/default/files/lrg_0.jpg\n",
      "http://pythonscraping.com/https://covers.oreillystatic.com/images/0636920078067/lrg.jpg\n",
      "http://pythonscraping.com/img/lrg%20(1).jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('downloaded/img/lrg%20(1).jpg', <http.client.HTTPMessage at 0x2798edc5d30>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "downloadDirectory = 'downloaded'\n",
    "baseUrl = 'http://pythonscraping.com'\n",
    "\n",
    "def getAbsoluteURL(baseUrl, source):\n",
    "    if source.startswith('http://www.'):\n",
    "        url = 'http://{}'.format(source[11:])\n",
    "    elif source.startswith('http://'):\n",
    "        url = source\n",
    "    elif source.startswith('www.'):\n",
    "        url = source[4:]\n",
    "        url = 'http://{}'.format(source)\n",
    "    else:\n",
    "        url = '{}/{}'.format(baseUrl, source)\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace('www.', '')\n",
    "    path = path.replace(baseUrl, '')\n",
    "    path = downloadDirectory+path\n",
    "    directory = os.path.dirname(path)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    return path\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "downloadList = bs.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download['src'])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "\n",
    "urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvFile = open('test.csv', 'w+')\n",
    "try:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(('number', 'number plus 2', 'number times 2'))\n",
    "    for i in range(10):\n",
    "        writer.writerow( (i, i+2, i*2))\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'cp949' codec can't encode character '\\u2011' in position 78: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a2079aab620e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'th'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mcsvRow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvRow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcsvFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'cp949' codec can't encode character '\\u2011' in position 78: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "# The main comparison table is currently the first table on the page\n",
    "# 비교테이블은 현재페이지의 첫번째 테이블\n",
    "table = bs.findAll('table',{'class':'wikitable'})[0]\n",
    "rows = table.findAll('tr')\n",
    "\n",
    "csvFile = open('editors.csv', 'wt+')\n",
    "writer = csv.writer(csvFile)\n",
    "try:\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "        writer.writerow(csvRow)\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n",
    "                       user='root', passwd='root', db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE scraping')\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def store(title, content):\n",
    "    cur.execute('INSERT INTO pages (title, content) VALUES (\"%s\", \"%s\")', (title, content))\n",
    "    cur.connection.commit()\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org'+articleUrl)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.find('h1').get_text()\n",
    "    content = bs.find('div', {'id':'mw-content-text'}).find('p').get_text()\n",
    "    store(title, content)\n",
    "    return bs.find('div', {'id':'bodyContent'}).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "try:\n",
    "    while len(links) > 0:\n",
    "         newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "         print(newArticle)\n",
    "         links = getLinks(newArticle)\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pymysql\n",
    "from random import shuffle\n",
    "\n",
    "conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock',\n",
    "                       user='root', passwd='root', db='mysql', charset='utf8')\n",
    "cur = conn.cursor()\n",
    "cur.execute('USE wikipedia')\n",
    "\n",
    "def insertPageIfNotExists(url):\n",
    "    cur.execute('SELECT * FROM pages WHERE url = %s', (url))\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute('INSERT INTO pages (url) VALUES (%s)', (url))\n",
    "        conn.commit()\n",
    "        return cur.lastrowid\n",
    "    else:\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "def loadPages():\n",
    "    cur.execute('SELECT * FROM pages')\n",
    "    pages = [row[1] for row in cur.fetchall()]\n",
    "    return pages\n",
    "\n",
    "def insertLink(fromPageId, toPageId):\n",
    "    cur.execute('SELECT * FROM links WHERE fromPageId = %s AND toPageId = %s', \n",
    "                  (int(fromPageId), int(toPageId)))\n",
    "    if cur.rowcount == 0:\n",
    "        cur.execute('INSERT INTO links (fromPageId, toPageId) VALUES (%s, %s)', \n",
    "                    (int(fromPageId), int(toPageId)))\n",
    "        conn.commit()\n",
    "def pageHasLinks(pageId):\n",
    "    cur.execute('SELECT * FROM links WHERE fromPageId = %s', (int(pageId)))\n",
    "    rowcount = cur.rowcount\n",
    "    if rowcount == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def getLinks(pageUrl, recursionLevel, pages):\n",
    "    if recursionLevel > 4:\n",
    "        return\n",
    "\n",
    "    pageId = insertPageIfNotExists(pageUrl)\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    links = bs.findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    links = [link.attrs['href'] for link in links]\n",
    "\n",
    "    for link in links:\n",
    "        linkId = insertPageIfNotExists(link)\n",
    "        insertLink(pageId, linkId)\n",
    "        if not pageHasLinks(linkId):\n",
    "            print(\"PAGE HAS NO LINKS: {}\".format(link))\n",
    "            pages.append(link)\n",
    "            getLinks(link, recursionLevel+1, pages)\n",
    "        \n",
    "        \n",
    "getLinks('/wiki/Kevin_Bacon', 0, loadPages()) \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "msg = MIMEText('The body of the email is here')\n",
    "\n",
    "msg['Subject'] = 'An Email Alert'\n",
    "msg['From'] = 'ryan@pythonscraping.com'\n",
    "msg['To'] = 'webmaster@pythonscraping.com'\n",
    "\n",
    "s = smtplib.SMTP('localhost')\n",
    "s.send_message(msg)\n",
    "s.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "def sendMail(subject, body):\n",
    "    msg = MIMEText(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] ='christmas_alerts@pythonscraping.com'\n",
    "    msg['To'] = 'ryan@pythonscraping.com'\n",
    "\n",
    "    s = smtplib.SMTP('localhost')\n",
    "    s.send_message(msg)\n",
    "    s.quit()\n",
    "\n",
    "bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n",
    "while(bs.find('a', {'id':'answer'}).attrs['title'] == 'NO'):\n",
    "    print('It is not Christmas yet.')\n",
    "    time.sleep(3600)\n",
    "    bs = BeautifulSoup(urlopen('https://isitchristmas.com/'), 'html.parser')\n",
    "sendMail('It\\'s Christmas!', \n",
    "         'According to http://itischristmas.com, it is Christmas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
